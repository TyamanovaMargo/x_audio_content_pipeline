import requests
import time
import json
from typing import List, Dict
import re

class BrightDataDownloader:
    def __init__(self, api_token: str):
        self.api_token = api_token
        self.base_url = "https://api.brightdata.com/datasets/v3"
        self.headers = {
            "Authorization": f"Bearer {api_token}",
            "Accept": "application/json",
            "User-Agent": "Python-Pipeline/1.0"
        }

    def wait_and_download_snapshot(self, snapshot_id: str, max_wait=600) -> List[Dict]:
        start = time.time()
        unknown_count = 0
        
        print(f"‚è≥ Waiting for snapshot {snapshot_id} to complete...")
        
        while (time.time() - start) < max_wait:
            status = self._check_status(snapshot_id)
            elapsed = int(time.time() - start)
            
            print(f"üîÑ [{elapsed}s] Snapshot {snapshot_id} status: {status}")
            
            if status == "completed":
                print("‚úÖ Snapshot ready! Downloading...")
                return self._download(snapshot_id)
            elif status == "unknown" or status == "error":
                unknown_count += 1
                print(f"‚ö†Ô∏è Status issue (attempt {unknown_count}/5)")
                
                # After 5 unknown statuses, try direct download
                if unknown_count >= 5:
                    print("üîÑ Attempting direct download despite status issues...")
                    try:
                        data = self._download(snapshot_id)
                        if data and len(data) > 0:
                            print("‚úÖ Direct download successful!")
                            return data
                        else:
                            print("‚ùå Direct download returned empty data")
                    except Exception as e:
                        print(f"‚ùå Direct download failed: {e}")
                    
                    unknown_count = 0  # Reset counter
                    
            elif status in ["failed", "expired", "cancelled"]:
                print(f"‚ùå Snapshot {status}")
                return []
            
            time.sleep(15)
        
        print(f"‚è∞ Timeout after {max_wait}s, attempting final download...")
        # Final attempt at download even if timeout
        try:
            return self._download(snapshot_id)
        except:
            return []

    def _check_status(self, snapshot_id: str) -> str:
        """Check snapshot status with enhanced debugging"""
        try:
            url = f"{self.base_url}/snapshot/{snapshot_id}"
            
            # Add debugging
            print(f"üîç Checking URL: {url}")
            print(f"üîç Headers: {self.headers}")
            
            response = requests.get(url, headers=self.headers, timeout=30)
            
            print(f"üîç Response Status: {response.status_code}")
            print(f"üîç Response Headers: {dict(response.headers)}")
            print(f"üîç Raw Response (first 300 chars): {response.text[:300]}")
            
            if not response.ok:
                print(f"‚ùå HTTP {response.status_code}: {response.text}")
                return "error"
            
            # Try to parse JSON response
            try:
                data = response.json()
                print(f"üîç Parsed JSON: {data}")
                
                if isinstance(data, dict):
                    status = data.get("status", "unknown")
                    print(f"üìä Extracted status: {status}")
                    return status
                elif isinstance(data, list) and len(data) > 0:
                    # Sometimes returns array with status in first element
                    first_item = data[0]
                    if isinstance(first_item, dict):
                        status = first_item.get("status", "unknown")
                        print(f"üìä Extracted status from array: {status}")
                        return status
                
                print(f"‚ö†Ô∏è Unexpected JSON structure: {type(data)}")
                return "unknown"
                
            except json.JSONDecodeError as e:
                print(f"‚ùå JSON decode error: {e}")
                print(f"‚ùå Response text: {response.text}")
                return "unknown"
                
        except requests.RequestException as e:
            print(f"‚ùå Request error: {e}")
            return "error"
        except Exception as e:
            print(f"‚ùå Unexpected error: {e}")
            return "error"

    def _download(self, snapshot_id: str) -> List[Dict]:
        """Download snapshot data with enhanced error handling"""
        try:
            url = f"{self.base_url}/snapshot/{snapshot_id}"
            params = {"format": "json", "compress": "false"}
            
            print(f"üì• Downloading from: {url}")
            print(f"üì• Parameters: {params}")
            
            response = requests.get(url, headers=self.headers, params=params, timeout=120)
            
            print(f"üì• Download Response Status: {response.status_code}")
            print(f"üì• Content-Type: {response.headers.get('Content-Type', 'unknown')}")
            print(f"üì• Content-Length: {response.headers.get('Content-Length', 'unknown')}")
            
            if not response.ok:
                print(f"‚ùå Download failed - HTTP {response.status_code}: {response.text[:200]}")
                return []
            
            # Parse response data
            data = self._safe_json_parse(response)
            
            if isinstance(data, list):
                print(f"üìä Downloaded {len(data)} profiles")
                return data
            elif isinstance(data, dict):
                # Sometimes wrapped in a data field
                if "data" in data and isinstance(data["data"], list):
                    print(f"üìä Downloaded {len(data['data'])} profiles from data field")
                    return data["data"]
                else:
                    print(f"üìä Downloaded 1 profile (single object)")
                    return [data]
            else:
                print(f"‚ùå Unexpected data format: {type(data)}")
                return []
                
        except Exception as e:
            print(f"‚ùå Download error: {e}")
            return []

    def _safe_json_parse(self, response):
        """Enhanced JSON parsing with NDJSON support"""
        try:
            # First try standard JSON parsing
            return response.json()
        except requests.exceptions.JSONDecodeError:
            # Handle NDJSON (newline-delimited JSON)
            text = response.text.strip()
            
            if not text:
                return []
            
            # Try parsing as NDJSON (multiple JSON objects separated by newlines)
            lines = [line.strip() for line in text.splitlines() if line.strip()]
            
            if len(lines) == 1:
                # Single line, try parsing as JSON
                try:
                    return json.loads(lines[0])
                except json.JSONDecodeError:
                    print(f"‚ùå Failed to parse single line JSON: {lines[0][:100]}...")
                    return []
            else:
                # Multiple lines, parse each as JSON
                parsed_objects = []
                for i, line in enumerate(lines):
                    try:
                        parsed_objects.append(json.loads(line))
                    except json.JSONDecodeError:
                        print(f"‚ö†Ô∏è Failed to parse line {i+1}: {line[:100]}...")
                        continue
                
                if parsed_objects:
                    return parsed_objects
                else:
                    print(f"‚ùå No valid JSON found in {len(lines)} lines")
                    return []

    def extract_external_links(self, profiles: List[Dict]) -> List[Dict]:
        """Extract external links from profiles with None-safe handling"""
        if not profiles:
            return []
            
        external_links = []
        
        for profile in profiles:
            if not profile:  # Skip if profile is None
                continue
                
            username = profile.get('username') or profile.get('screen_name', '')
            
            # Search for external links in various fields
            link_fields = ['external_link', 'url', 'website', 'profile_external_link', 'bio_link']
            
            found_link = None
            for field in link_fields:
                link = profile.get(field)
                
                # Safe None handling
                if link is None:
                    link = ''
                else:
                    link = str(link).strip()  # Convert to string and strip
                
                if link and link.startswith('http'):
                    found_link = link
                    break
            
            if found_link:
                # Safe handling for description field too
                description = profile.get('description')
                bio = description[:100] if description else ''
                
                external_links.append({
                    'username': username,
                    'profile_name': profile.get('profile_name', ''),
                    'url': found_link,
                    'followers': profile.get('followers', 0),
                    'bio': bio
                })
        
        print(f"üîó Extracted {len(external_links)} external links from {len(profiles)} profiles")
        return external_links
 
    def extract_external_links(self, profiles: List[Dict]) -> List[Dict]:
        """Extract external links from profiles with enhanced debugging"""
        if not profiles:
            return []
            
        external_links = []
        
        # –î–æ–±–∞–≤–∏–º —Å—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        total_profiles = len(profiles)
        skipped_no_profile = 0
        skipped_no_username = 0
        skipped_no_links = 0
        processed_profiles = 0
        
        for i, profile in enumerate(profiles):
            if not profile:  # Skip if profile is None
                skipped_no_profile += 1
                continue
            
            # –û—Ç–ª–∞–¥–∫–∞: –ø–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ñ–∏–ª–µ–π
            if i < 3:
                print(f"üîç Profile {i} keys: {list(profile.keys())}")
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–ª–µ–π –¥–ª—è username
            username_candidates = [
                profile.get('username'),
                profile.get('screen_name'),
                profile.get('handle'),
                profile.get('user_name'),
                profile.get('account_name'),
                profile.get('name'),
                profile.get('display_name')
            ]
            
            username = None
            for candidate in username_candidates:
                if candidate and str(candidate).strip():
                    username = str(candidate).strip()
                    break
            
            # –ï—Å–ª–∏ username –≤—Å–µ –µ—â–µ –ø—É—Å—Ç–æ–π, –ø–æ–ø—Ä–æ–±—É–µ–º ID
            if not username:
                user_id = profile.get('id') or profile.get('user_id') or profile.get('profile_id')
                if user_id:
                    username = str(user_id)  # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å "user_" –∑–¥–µ—Å—å —Ç–æ–∂–µ
                else:
                    print(f"‚ö†Ô∏è Profile {i} has no username/ID: {list(profile.keys())[:5]}")
                    skipped_no_username += 1
                    continue
            
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å "user_" –∏–∑ username –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å[107][108]
            if username.startswith("user_"):
                username = username.removeprefix("user_")  # –î–ª—è Python 3.9+
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π Python:
                # username = username[5:] if username.startswith("user_") else username
            
            # Search for external links in various fields
            link_fields = ['external_link', 'url', 'website', 'profile_external_link', 'bio_link', 'link']
            
            found_link = None
            for field in link_fields:
                link = profile.get(field)
                
                if link is None:
                    link = ''
                else:
                    link = str(link).strip()
                
                if link and link.startswith('http'):
                    found_link = link
                    break
            
            if found_link:
                description = profile.get('description')
                bio = description[:100] if description else ''
                
                external_links.append({
                    'username': username,  # –¢–µ–ø–µ—Ä—å –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ "user_"
                    'profile_name': profile.get('profile_name') or profile.get('name') or profile.get('display_name') or '',
                    'url': found_link,
                    'followers': profile.get('followers', 0),
                    'bio': bio
                })
                processed_profiles += 1
            else:
                skipped_no_links += 1
        
        # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"üìä Processing Summary:")
        print(f"   Total profiles: {total_profiles}")
        print(f"   Skipped (no profile data): {skipped_no_profile}")
        print(f"   Skipped (no username): {skipped_no_username}")
        print(f"   Skipped (no external links): {skipped_no_links}")
        print(f"   Successfully processed: {processed_profiles}")
        print(f"üîó Extracted {len(external_links)} external links")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö username
        if external_links:
            sample_usernames = [link['username'] for link in external_links[:5]]
            print(f"üìù Sample usernames: {sample_usernames}")
        
        return external_links
